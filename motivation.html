<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Motivation</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.css" rel="stylesheet">

    <!-- Custom CSS for the '3 Col Portfolio' Template -->
    <link href="css/3-col-portfolio.css" rel="stylesheet">
  </head>

  <body>
    <nav class="navbar navbar-fixed-top navbar-inverse" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Stop Sign Tampering Detection</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse navbar-ex1-collapse">
          <ul class="nav navbar-nav">
            <li><a href="motivation.html">Motivation</a></li>
            <li><a href="methodology.html">Methodology</a></li>
            <li><a href="implementation.html">Implementation</a></li>
            <li><a href="results.html">Results</a></li>
            <li><a href="challenges.html">Challenges & Future</a></li>
            <li><a href="slides.pdf">Project Slides</a></li>
          </ul>
        </div><!-- /.navbar-collapse -->
      </div><!-- /.container -->
    </nav>

    <div class="container">
      <div class="row">
        <div class="col-lg-12">
          <h1 class="page-header">Motivation</h1>
        </div>
      </div>

      <div class="row">
        <div class="col-lg-4">
          <img src="assets/misclass-1.jpg" alt="Stop Sign Misclassifications">
        </div>
        <div class="col-lg-4">
          <img src="assets/misclass-2.jpg" alt="Stop Sign Misclassifications">
        </div>
        <div class="col-lg-4">
          <img src="assets/misclass-3.jpg" alt="Stop Sign Misclassifications">
        </div>
      </div>
      
      <div class="row">
        <div class="col-lg-12">
          <h3>Adversarial Attack Article</h3>
          <p>
            In the beginnings of planning our project, we knew we wanted to do something related to autonomous vehicles and how they percieve the world. At first, we planned on creating a lane line quality analysis alogirthm. However, we completely shifted our plans when we read <a href="https://arstechnica.com/cars/2017/09/hacking-street-signs-with-stickers-could-confuse-self-driving-cars/">this article from ArsTechnica</a> about adversarial attacks on stop signs. With only a few pieces of tape or a pattern of "noise", the researchers could cause stop signs to be misclassified a high percentage of the time. And of these misclassifications, some of them were seen as 45mph signs.
          </p>
          <p>To quote from the article: </p>
          <blockquote>
            Ivan Evtimov—a grad student at the University of Washington—and some colleagues first trained a deep neural network to recognize different US road signs. Then, they created an algorithm that generated changes to the signs that human eyes find innocuous, but which changed the meaning when a sign was read by the AI classifier they just trained. Evtimov and his co-authors propose two different ways to hack a street sign, either by printing out an altered copy that you cover the existing sign with or by just making small additions with stickers. 
          </blockquote>
          <p>
            This style of adversarial attack is known as a White Box attack. In such an attack, the attacker has direct privileges and access to the model's settings and parameters; contrast that with a Black Box attack in which the attacker only has limited access to the model and can only access it through the input and output. While a Black Box attack is the more realistic of the two, this White Box attack serves as more a proof of concept. It shows that with a few simple alterations such as sticking pieces of tape on a stop sign, an attacker could at best cause some slight annoyances and at worst a horrible traffic accident.
          </p>

          <h3>Project Proposal</h3>
          <p>
            In response to this article, our team decided that we wanted to see how simple it would be to create a convolutional neural network to classifiy stop signs, but more specifically, to see how robust we could make it to alerations such as graffiti or even just wear and tear from being out in the elements. Our project wasn't meant as a defense to the attack proposed in the above article, but rather our own proof of concept to see if CNNs could be used for tamper detection.
          </p>
          <p>
            To carry the idea further past the scope of our project, we surmised that if tampering was detected, the suspect stop sign could be sent further along down the chain to a more robust classification algorithm (but perhaps requiring more compute) that could take into account other environment factors and cues to determine the true nature of the sign. For example, it could consider if there is a stopbar present in the scene, or if the sign shape was octogonal.
          </p>
          <p>To find a detailed explanation of our project methodology, please visit the <a href="methodology.html">Methodology</a> tab.</p>
        </div>
      </div>
    </div><!-- /.container -->
    

    <div class="container footer">
      <hr>
      <a href="https://github.com/CardboardKungfu/cardboardkungfu.github.io">Github Repository</a>
    </div><!-- /.container -->

    <!-- JavaScript -->
    <script src="js/jquery-1.10.2.js"></script>
    <script src="js/bootstrap.js"></script>
  </body>
</html>